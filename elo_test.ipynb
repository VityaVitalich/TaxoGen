{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('main.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = ['SD3', 'SDXL', 'SDXL_Turbo', 'Kandinsky', 'PixArt_Sigma', 'Playground', 'IF', 'Openjourney', 'Hunyuan-DiT', 'SD_V1.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model_pair(models_list):\n",
    "    return random.sample(models_list, 2)\n",
    "\n",
    "# Adding the sampled pairs to the dataframe\n",
    "df[['model_1', 'model_2']] = df.apply(lambda row: pd.Series(sample_model_pair(models_list)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hunyuan-DiT     701\n",
       "IF              685\n",
       "Kandinsky       699\n",
       "Openjourney     708\n",
       "PixArt_Sigma    662\n",
       "Playground      637\n",
       "SD3             637\n",
       "SDXL            646\n",
       "SDXL_Turbo      661\n",
       "SD_V1.5         704\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['model_1'].value_counts() + df['model_2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.4, 0.4, 0.1, 0.1]  # Weights for values 0, 1, 2, 3, 4\n",
    "values = [0, 1, 2, 3]\n",
    "\n",
    "df['assessor_vitya'] = random.choices(values, weights=weights, k=len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1382\n",
       "0    1333\n",
       "2     332\n",
       "3     323\n",
       "Name: assessor_vitya, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['assessor_vitya'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('main.tsv', sep='\\t')\n",
    "\n",
    "initial_elo = 1200\n",
    "elo_ratings = {model: initial_elo for model in set(df['model_1']).union(df['model_2'])}\n",
    "\n",
    "# Function to calculate expected score\n",
    "def expected_score(rating_a, rating_b):\n",
    "    return 1 / (1 + 10**((rating_b - rating_a) / 400))\n",
    "\n",
    "# Function to update ELO ratings\n",
    "def update_elo(rating_a, rating_b, score_a, k=32):\n",
    "    expected_a = expected_score(rating_a, rating_b)\n",
    "    new_rating_a = rating_a + k * (score_a - expected_a)\n",
    "    new_rating_b = rating_b + k * ((1 - score_a) - (1 - expected_a))\n",
    "    return new_rating_a, new_rating_b\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    model_1 = row['model_1']\n",
    "    model_2 = row['model_2']\n",
    "    outcome = row['assessor_vitya']\n",
    "    \n",
    "    if outcome == 0:  # model_1 wins\n",
    "        score_1 = 1\n",
    "        score_2 = 0\n",
    "    elif outcome == 1:  # model_2 wins\n",
    "        score_1 = 0\n",
    "        score_2 = 1\n",
    "    elif outcome == 2:  # tie\n",
    "        score_1 = 0.5\n",
    "        score_2 = 0.5\n",
    "    elif outcome == 3:  # both models are bad\n",
    "        continue  # No change in ratings for this outcome\n",
    "    \n",
    "    rating_1 = elo_ratings[model_1]\n",
    "    rating_2 = elo_ratings[model_2]\n",
    "    \n",
    "    new_rating_1, new_rating_2 = update_elo(rating_1, rating_2, score_1)\n",
    "    \n",
    "    elo_ratings[model_1] = new_rating_1\n",
    "    elo_ratings[model_2] = new_rating_2\n",
    "\n",
    "    # Convert ratings_changes to dataframe\n",
    "    changes_df = pd.DataFrame.from_dict(ratings_changes, orient='index').fillna(0).T\n",
    "\n",
    "    # Calculate sandwich robust standard errors\n",
    "    robust_cov = cov_hc0(changes_df)\n",
    "    robust_std_errors = np.sqrt(np.diag(robust_cov))\n",
    "\n",
    "    # Calculate confidence intervals\n",
    "    z_score = 1.96  # For 95% confidence interval\n",
    "    confidence_intervals = {\n",
    "        model: (elo_ratings[model] - z_score * robust_std_errors[i], elo_ratings[model] + z_score * robust_std_errors[i])\n",
    "        for i, model in enumerate(elo_ratings)\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for model, rating in elo_ratings.items():\n",
    "            ci = confidence_intervals[model]\n",
    "            f.write(f\"{model}: {rating} (95% CI: {ci[0]:.2f} - {ci[1]:.2f})\\n\")\n",
    "\n",
    "# Convert the ELO ratings dictionary to a dataframe for display\n",
    "elo_df = pd.DataFrame(list(elo_ratings.items()), columns=['Model', 'ELO Rating'])\n",
    "elo_df = elo_df.sort_values(by='ELO Rating', ascending=False).reset_index(drop=True)\n",
    "\n",
    "elo_txt_path = \"elo_ratings.txt\"\n",
    "with open(elo_txt_path, 'w') as f:\n",
    "    for model, rating in elo_ratings.items():\n",
    "        f.write(f\"{model}: {rating}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from statsmodels.stats.sandwich_covariance import cov_hc0\n",
    "\n",
    "def expected_score(rating_a, rating_b):\n",
    "    return 1 / (1 + 10**((rating_b - rating_a) / 400))\n",
    "\n",
    "def update_elo(rating_a, rating_b, score_a, k=32):\n",
    "    expected_a = expected_score(rating_a, rating_b)\n",
    "    new_rating_a = rating_a + k * (score_a - expected_a)\n",
    "    new_rating_b = rating_b + k * ((1 - score_a) - (1 - expected_a))\n",
    "    return new_rating_a, new_rating_b\n",
    "\n",
    "elo_ratings = {model: initial_elo for model in set(df['model_1']).union(df['model_2'])}\n",
    "label_column='assessor_vitya'\n",
    "output_file = 'ELO_test.txt'\n",
    "ratings_changes = {model: [] for model in elo_ratings.keys()}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    model_1 = row['model_1']\n",
    "    model_2 = row['model_2']\n",
    "    outcome = row[label_column]\n",
    "\n",
    "    if outcome == 0:  # model_1 wins\n",
    "        score_1 = 1\n",
    "        score_2 = 0\n",
    "    elif outcome == 1:  # model_2 wins\n",
    "        score_1 = 0\n",
    "        score_2 = 1\n",
    "    elif outcome == 2:  # tie\n",
    "        score_1 = 0.5\n",
    "        score_2 = 0.5\n",
    "    elif outcome == 3:  # both models are bad\n",
    "        continue  # No change in ratings for this outcome\n",
    "\n",
    "    rating_1 = elo_ratings[model_1]\n",
    "    rating_2 = elo_ratings[model_2]\n",
    "\n",
    "    new_rating_1, new_rating_2 = update_elo(rating_1, rating_2, score_1)\n",
    "\n",
    "    ratings_changes[model_1].append(new_rating_1 - rating_1)\n",
    "    ratings_changes[model_2].append(new_rating_2 - rating_2)\n",
    "\n",
    "    elo_ratings[model_1] = new_rating_1\n",
    "    elo_ratings[model_2] = new_rating_2\n",
    "\n",
    "# Calculate standard deviation of rating changes\n",
    "ratings_std = {model: np.std(changes) for model, changes in ratings_changes.items()}\n",
    "\n",
    "# Calculate confidence intervals\n",
    "z_score = 1.96  # For 95% confidence interval\n",
    "confidence_intervals = {\n",
    "    model: (elo_ratings[model] - z_score * ratings_std[model], elo_ratings[model] + z_score * ratings_std[model])\n",
    "    for model in elo_ratings\n",
    "}\n",
    "\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for model, rating in elo_ratings.items():\n",
    "        ci = confidence_intervals[model]\n",
    "        f.write(f\"{model}: {rating} (95% CI: {ci[0]:.2f} - {ci[1]:.2f})\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.24.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.12.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.5.3)\n",
      "Collecting patsy>=0.5.6 (from statsmodels)\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
      "Downloading statsmodels-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.9/233.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: patsy, statsmodels\n",
      "Successfully installed patsy-0.5.6 statsmodels-0.14.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bjlkeng/Bradley-Terry-Model/blob/master/update_model.py\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "DUMMY_PLAYER = 'DUMMY PLAYER'\n",
    "\n",
    "def extract_game_data(df):\n",
    "    df1 = deepcopy(df[df['assessor_vitya'] < 2])\n",
    "    df1['Player A'] = df1['model_1']\n",
    "    df1['Player B'] = df1['model_2']\n",
    "    df1['Wins A'] = df1['assessor_vitya'] == 0\n",
    "    df1['Wins B'] = df1['assessor_vitya'] == 1\n",
    "\n",
    "    # assert all(c in df.columns for c in ['Date', 'Player A', 'Player B', 'Wins A', 'Wins B']), \\\n",
    "    #     'Expecting columns Date, Player A, Player B, Wins A, Wins B'\n",
    "\n",
    "   # df['Date'] = df['Date'].astype(datetime)\n",
    "    df1['Wins A'] = df1['Wins A'].astype(int)\n",
    "    df1['Wins B'] = df1['Wins B'].astype(int)\n",
    "\n",
    "    df1 = df1.drop(columns = df.columns)\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "\n",
    "def add_dummy_games(game_data, alpha=1):\n",
    "    ''' Regularizes the estimate by adding games against a dummy player.\n",
    "\n",
    "        :param alpha: regularization parameter, number dummy wins/loses to add\n",
    "    '''\n",
    "    players = sorted(list(set(game_data['Player A']) | set(game_data['Player B'])))\n",
    "\n",
    "    # Add dummy games\n",
    "    dummy_data = [[p, DUMMY_PLAYER, alpha, alpha] for p in players]\n",
    "    df = pd.DataFrame(dummy_data, columns=game_data.columns)\n",
    "    df = pd.concat([game_data, df])\n",
    "    df\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_rank_scores(game_data, max_iters=1000, error_tol=1e-3):\n",
    "    ''' Computes Bradley-Terry using iterative algorithm\n",
    "\n",
    "        See: https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model\n",
    "    '''\n",
    "    # Do some aggregations for convenience\n",
    "    # Total wins per player\n",
    "    winsA = game_data.groupby('Player A').agg(sum)['Wins A'].reset_index()\n",
    "    winsA = winsA[winsA['Wins A'] > 0]\n",
    "    winsA.columns = ['Player', 'Wins']\n",
    "    winsB = game_data.groupby('Player B').agg(sum)['Wins B'].reset_index()\n",
    "    winsB = winsB[winsB['Wins B'] > 0]\n",
    "    winsB.columns = ['Player', 'Wins']\n",
    "    wins = pd.concat([winsA, winsB]).groupby('Player').agg(sum)['Wins']\n",
    "\n",
    "    # Total games played between pairs\n",
    "    num_games = Counter()\n",
    "    for index, row in game_data.iterrows():\n",
    "        key = tuple(sorted([row['Player A'], row['Player B']]))\n",
    "        total = sum([row['Wins A'], row['Wins B']])\n",
    "        num_games[key] += total\n",
    "\n",
    "    # Iteratively update 'ranks' scores\n",
    "    players = sorted(list(set(game_data['Player A']) | set(game_data['Player B'])))\n",
    "    ranks = pd.Series(np.ones(len(players)) / len(players), index=players)\n",
    "    for iters in range(max_iters):\n",
    "        oldranks = ranks.copy()\n",
    "        for player in ranks.index:\n",
    "            denom = np.sum(num_games[tuple(sorted([player, p]))]\n",
    "                           / (ranks[p] + ranks[player])\n",
    "                           for p in ranks.index if p != player)\n",
    "            ranks[player] = 1.0 * wins[player] / denom\n",
    "\n",
    "        ranks /= sum(ranks)\n",
    "\n",
    "        if np.sum((ranks - oldranks).abs()) < error_tol:\n",
    "            break\n",
    "\n",
    "    if np.sum((ranks - oldranks).abs()) < error_tol:\n",
    "        print(f\" * Converged after {iters} iterations.\")\n",
    "    else:\n",
    "        print(f\" * Max iterations reached ({max_iters} iters).\")\n",
    "\n",
    "    del ranks[DUMMY_PLAYER]\n",
    "\n",
    "    # Scale logarithm of score to be between 1 and 1000\n",
    "    ranks = ranks.sort_values(ascending=False) \\\n",
    "                 .apply(lambda x: np.log1p(1000 * x) / np.log1p(1000) * 1000) \\\n",
    "                 .astype(int) \\\n",
    "                 .clip(1)\n",
    "\n",
    "    return ranks\n",
    "\n",
    "df = pd.read_csv('main.tsv', sep='\\t')\n",
    "\n",
    "df1 = extract_game_data(df)\n",
    "games = add_dummy_games(df1)\n",
    "ranks = compute_rank_scores(games)\n",
    "\n",
    "BT_txt_path = \"BT_ratings.txt\"\n",
    "with open(BT_txt_path, 'w') as f:\n",
    "    for model, rating in ranks.items():\n",
    "        f.write(f\"{model}: {rating}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kandinsky 666\n",
      "Hunyuan-DiT 662\n",
      "SD3 661\n",
      "IF 660\n",
      "SDXL 659\n",
      "Openjourney 654\n",
      "PixArt_Sigma 646\n",
      "SDXL_Turbo 644\n",
      "SD_V1.5 643\n",
      "Playground 642\n"
     ]
    }
   ],
   "source": [
    "for model_name, rating in ranks.items():\n",
    "    print(model_name, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Converged after 5 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218453/3421610540.py:56: FutureWarning: The operation <built-in function sum> failed on a column. If any error is raised, this will raise an exception in a future version of pandas. Drop these columns to avoid this warning.\n",
      "  winsA = game_data.groupby('Player A').agg(sum)['Wins A'].reset_index()\n",
      "/tmp/ipykernel_218453/3421610540.py:59: FutureWarning: The operation <built-in function sum> failed on a column. If any error is raised, this will raise an exception in a future version of pandas. Drop these columns to avoid this warning.\n",
      "  winsB = game_data.groupby('Player B').agg(sum)['Wins B'].reset_index()\n",
      "/tmp/ipykernel_218453/3421610540.py:77: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  denom = np.sum(num_games[tuple(sorted([player, p]))]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kandinsky       666\n",
       "Hunyuan-DiT     662\n",
       "SD3             661\n",
       "IF              660\n",
       "SDXL            659\n",
       "Openjourney     654\n",
       "PixArt_Sigma    646\n",
       "SDXL_Turbo      644\n",
       "SD_V1.5         643\n",
       "Playground      642\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
